<html>

<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8" />
  <title>Hao Li (Leo Li)</title>
  <link href="./css/style.css" rel="stylesheet" media="all" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons/css/academicons.min.css" />
  <script type="text/javascript" src="https://code.jquery.com/jquery-2.2.0.min.js"></script>
  <script src="https://kit.fontawesome.com/bd3becbd8d.js" crossorigin="anonymous"></script>

  <script type="text/javascript">
    window.onload = choosePic;
    var myPix = new Array("./images/portrait.jpg");
    function choosePic() {
      var randomNum = Math.floor(Math.random() * myPix.length);
      document.getElementById("myPicture").src = myPix[randomNum];
    }
    function lastUpdate() {
      var x = document.lastModified.substr(0, 10);
      document.getElementById("demo").innerHTML = x;
    }
  </script>

  <script type="text/javascript" src="./js/hidebib.js"></script>
  <script type="text/javascript" src="./js/loadtxt.js"></script>
</head>

<body onload="lastUpdate()">
  <div class="content">
    <div id="container">
      <table style="margin: 0 auto; text-align: center; width: 100%">
        <tr>
          <td style="text-align: center">
            <img
              id="myPicture"
              src="./images/portrait.jpg"
              style="border-radius: 20%; width: 300px;"
              alt="Portrait"
            />
          </td>
          <td>
            <div id="DocInfo" style="text-align: center;">
              <div id="intro">
                <h1>Hao Li (Leo Li)</h1>
                <a style="color: black" href="https://en.nwpu.edu.cn/">
                  Northwestern Polytechnical University
                </a>
                <br /><br />
                <a href="mailto:lifugan_10027@outlook.com">
                  <span
                    class="fa fa-envelope"
                    style="color: rgb(59, 59, 63); font-size: 20px"
                  >
                    <span style="font-family: Optima Bold">lifugan_10027</span>
                    <span
                      class="fa fa-at"
                      style="color: rgb(59, 59, 63); font-size: 20px"
                    >
                      <span style="font-family: Optima Bold">outlook.com</span>
                    </span>
                  </span>
                </a>
                <br />
              </div>
              <br />
              <a style="color: black" href="https://github.com/lifuguan">
                <span class="fa fa-github fa-xl"></span>
              </a>
              <a
                style="color: black"
                href="https://scholar.google.com/citations?user=4dokjDoAAAAJ"
              >
                <span class="ai ai-google-scholar fa-xl"></span>
              </a>
              <br />
            </div>
          </td>
        </tr>
      </table>
      
      <table>
        <tr>
          <td>
            <br />
            <h1>Biography</h1>

            I am currently a fourth-year (2022-now) Ph.D. student in the School of Automation at 
            <a href="https://en.nwpu.edu.cn/">Northwestern Polytechnical University (NPU)</a>,
            supervised by <a href="https://teacher.nwpu.edu.cn/zdw2006yyy">Prof. Dingwen Zhang</a> and <a href="https://scholar.google.com/citations?user=xrqsoesAAAAJ">Prof. Junwei Han (IEEE Fellow)</a>.

            <span style="color: red;">
              Meanwhile, I am a visiting student at <a href="https://www.ntu.edu.sg">Nanyang Technological University (NTU)</a> since April 2025, supervised by Prof. <a href="https://liuziwei7.github.io">Ziwei Liu</a>.
            </span></span><br />

My research interests lie in <u>3D Vision</u>, <u>Embodied AI</u>, and <u>Multi-Modal Model</u>.<br /><br />
            

            <span style="color: red;">
              I also join the LongCat Foundation Group, <a href="https://www.meituan.com">Meituan, Inc.</a> as Research Intern (北斗人才计划).
            </span>
            
            Before that, I worked as a research intern at 
            <a href="https://stepfun.com">StepFun Inc.</a>, led by <a href="https://openreview.net/profile?id=~Xuanyang_Zhang2">Xuanyang Zhang</a> and <a href="https://www.skicyyu.org/">Dr. Gang Yu</a>.
            From 2023 to 2024, I worked as a research intern at the Robotcis Team, 
            <a href="https://www.bytedance.com/">ByteDance AI Lab</a>, under the mentorship of 
            <a href="https://minghanqin.github.io/">Minghan Qin</a>. I also interned with the VIS, 
            at <a href="https://vis.baidu.com/#/">Baidu Inc.</a>, guided by 
            <a href="https://chenming-wu.github.io/">Dr. Chenming Wu</a> and  <a href="https://jingdongwang2017.github.io/">Jingdong Wang (IEEE Fellow)</a>.

            In 2022 to 2023,I was a research intern at <a href="https://www.shlab.org.cn/">Zhejiang Lab</a>, 
            leading with <a href="https://scholar.google.com/citations?user=PKFAv-cAAAAJ">Prof. Lechao Cheng</a> .
            <br /><br />

          </td>
        </tr>
      </table>

      <div class="logo">
        <a href="https://en.nwpu.edu.cn/"><img  src="./images/npu.png" /></a>
        <a href="https://www.ntu.edu.sg/"><img style="border-radius: 0%" src="./images/NTU_Logo.jpg" /></a>
        <a href="https://www.meituan.com"><img style="border-radius: 0%" src="./images/meituan.png" /></a>
        <a href="https://www.stepfun.com/"><img style="border-radius: 0%" src="./images/stepfun.png" /></a>
        <a href="https://bytedance.com"><img  style="border-radius: 0%" src="./images/bytedance-ht.png" /></a>
        <a href="https://vis.baidu.com"><img style="border-radius: 0%" src="./images/baidu-logo.png" /></a>
        <a href="https://www.zhejianglab.com"><img style="border-radius: 0%" src="./images/zhejiang lab.png" /></a>
      </div>


      <h1>News</h1>
      [2025/06] <a href="#">STRIDER</a> got accepted by <strong style="color: brown">NeurIPS 2025</strong>! &#127881;<br>
      [2025/06] <a href="#">LangScene-X</a> and <a href="#">CityGS-X</a> got accepted by <strong style="color: brown">ICCV 2025</strong>! &#127881;<br>
      [2025/05] <a href="https://arxiv.org/abs/2505.07747">Step1X-3D</a> released by StepFun! &#127881;<br>
      [2025/04] Visiting student at <a href="https://www.ntu.edu.sg">NTU</a>, supervised by <a href="https://liuziwei7.github.io">Prof. Ziwei Liu</a>!<br>
      [2025/03] <a href="https://3d-aigc.github.io/VDG/">VDG</a> got accepted by <strong style="color: brown">IEEE RA-L 2025</strong>! &#127881;<br>
      [2025/02] <a href="#">DGTR</a> got accepted by <strong style="color: brown">ICRA 2025</strong>! &#127881;<br>
      [2025/01] Joining AIGC Group, StepFun Inc., led by <a href="https://openreview.net/profile?id=~Xuanyang_Zhang2">Xuanyang Zhang</a> and <a href="https://www.skicyyu.org/">Dr. Gang Yu</a>!<br>
      [2024/10] <a href="https://3d-aigc.github.io/XLD">XLD</a> got accepted by <strong style="color: brown">3DV 2025</strong>! &#127881;<br>
      [2024/08] Joining ByteDance AI Lab, led by <a href="https://minghanqin.github.io/">Minghan Qin</a>!<br>
      [2024/08] Invited talk on <a href="https://mp.weixin.qq.com/s/rGh-zI0zYaJKZ0EWIjyy7w">GAMES Webinar</a>.<br>
      [2024/07] <a href="https://3d-aigc.github.io/GGRt">GGRt</a> got accepted by <strong style="color: brown">ECCV 2024</strong>!<br>
      [2024/02] <a href="https://github.com/lifuguan/gp-nerf">GP-NeRF</a> got accepted by <strong style="color: brown">CVPR 2024</strong> and selected as <strong style="color: brown">Highlight (top 3.8%)</strong>! &#127881;<br>
      [2024/02] <a href="https://ltgccode.github.io/">LTGC</a> got accepted by <strong style="color: brown">CVPR 2024</strong> and selected as <strong style="color: brown">Oral (top 0.8%)</strong>! &#127881;<br>
      [2024/01] Invited talk on <a href="https://mp.weixin.qq.com/s/htNrZaDuhwez03kqJLwjpg">3D视觉工坊</a>.<br>
      [2023/12] Joining VIS, Baidu, Inc. as Research Intern, led by <a href="https://chenming-wu.github.io/">Dr. Chenming Wu</a> and  <a href="https://jingdongwang2017.github.io/">Jingdong Wang (IEEE Fellow)</a>!<br>
      [2023/11] <a href="https://ieeexplore.ieee.org/document/10367821">ASDT</a> got accepted by <strong style="color: brown">TIP 2024</strong>! &#127881;<br>
      [2023/11] <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Boosting_Low-Data_Instance_Segmentation_by_Unsupervised_Pre-Training_With_Saliency_Prompt_CVPR_2023_paper.pdf">Saliency Prompt</a> got accepted by <strong style="color: brown">CVPR 2023</strong>! &#127881;<br>
      [2023/12] Joining Zhejiang Lab as Research Intern, led by <a href="https://scholar.google.com/citations?user=PKFAv-cAAAAJ">Prof. Lechao Cheng</a>!<br>
      <br /><br />

      <h1 id="citation">Publications</h1>
      
      <table class="pub_table">

        <tr id="step1x3d2025">
          <td class="pub_td1">
            <img
              src="images/step1x-3d-teaser.png"
              width="180"
              height="120"
              style="display: block; margin: 0 auto; border-radius: 5px;"
              alt="Step1X-3D Image"
            />
          </td>
          <td class="pub_td2">
            <b>Step1X-3D: Towards High-Fidelity and Controllable Generation of Textured 3D Assets</b>
            <br /><br />
            <u>Weiyu Li</u>, <u>Xuanyang Zhang</u>, <u>Zheng Sun</u>, <u>Di Qi</u>,
            <strong>Hao Li</strong>, <u>Weiwei Cheng</u>, <u>Wanggui Cai</u>, <u>Shun Wu</u>, <u>Jie Liu</u>, <u>Ziwei Wang</u>, <u>Gang Yu</u>
            <br />
            <div style="margin-top: 10px; margin-bottom: 10px">
              <i><strong style="color: brown">Tech Report 2025</strong></i>
            </div>
            <a class="box" href="https://step-1x.github.io/step1x-3d.github.io/">
              <i class="fas fa-home" aria-hidden="true">&nbsp;</i>Home&nbsp;
            </a>
            <a class="box" href="https://arxiv.org/abs/2505.07747">
              <i class="fa fa-file-lines" aria-hidden="true">&nbsp;</i>Paper
            </a>
            <a href="https://github.com/stepfun-ai/Step1X-3D">
              <i class="fa fa-github" aria-hidden="true">&nbsp;</i>Code&nbsp;
            </a>
          </td>
        </tr>

        <tr id="omnivggt2025">
          <td class="pub_td1">
            <img
              src="images/omnivggt.png"
              width="180"
              height="120"
              style="display: block; margin: 0 auto; border-radius: 5px;"
              alt="OmniVGGT Image"
            />
          </td>
          <td class="pub_td2">
            <b>OmniVGGT: Omni-Modality Driven Visual Geometry Grounded Transformer</b>
            <br /><br />
            <u>Haoyi Peng†</u>,
            <strong>Hao Li†</strong>, <u>Yalun Dai</u>, <u>Yao Lan</u>, <u>Yufei Luo</u>, <u>Tianyu Qi</u>, <u>Zhizheng Zhang</u>, <u>Yuxuan Zhan</u>, <u>Junran Peng</u>, <u>Wu Xu</u>, <u>Ziwei Liu</u>
            <br />
            <div style="margin-top: 10px; margin-bottom: 10px">
              <i><strong style="color: brown">arXiv 2025</strong></i>
            </div>
            <a class="box" href="https://arxiv.org/abs/2511.10560">
              <i class="fa fa-file-lines" aria-hidden="true">&nbsp;</i>Paper
            </a>
          </td>
        </tr>

        <tr id="iggt2025">
          <td class="pub_td1">
            <img
              src="images/iggt.png"
              width="180"
              height="120"
              style="display: block; margin: 0 auto; border-radius: 5px;"
              alt="IGGT Image"
            />
          </td>
          <td class="pub_td2">
            <b>IGGT: Instance-Grounded Geometry Transformer for Semantic 3D Reconstruction</b>
            <br /><br />
            <strong>Hao Li</strong>, <u>Zhengyu Zou</u>, <u>Fangfu Liu</u>, <u>Xuanyang Zhang</u>, <u>Fangzhou Hong</u>, <u>Yueqi Cao</u>, <u>Yao Lan</u>, <u>Ming Zhang</u>, <u>Gang Yu</u>, <u>Ziwei Liu</u>
            <br />
            <div style="margin-top: 10px; margin-bottom: 10px">
              <i><strong style="color: brown">arXiv 2025</strong></i>
            </div>
            <a class="box" href="https://arxiv.org/abs/2510.22706">
              <i class="fa fa-file-lines" aria-hidden="true">&nbsp;</i>Paper
            </a>
          </td>
        </tr>

        <tr id="spatial2actions2025">
          <td class="pub_td1">
            <img
              src="images/from spatial to action.png"
              width="180"
              height="120"
              style="display: block; margin: 0 auto; border-radius: 5px;"
              alt="From Spatial to Actions Image"
            />
          </td>
          <td class="pub_td2">
            <b>From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Prior</b>
            <br /><br />
            <u>Zhizheng Zhang</u>,
            <strong>Hao Li</strong>, <u>Yalun Dai</u>, <u>Ziao Zhu</u>, <u>Lin Zhou</u>, <u>Chen Liu</u>, <u>Dong Wang</u>, <u>Francis E.H. Tay</u>, <u>Shuicheng Chen</u>, <u>Ziwei Liu</u>
            <br />
            <div style="margin-top: 10px; margin-bottom: 10px">
              <i><strong style="color: brown">arXiv 2025</strong></i>
            </div>
            <a class="box" href="https://arxiv.org/abs/2510.17439">
              <i class="fa fa-file-lines" aria-hidden="true">&nbsp;</i>Paper
            </a>
          </td>
        </tr>

        <tr id="strider2025">
          <td class="pub_td1">
            <img
              src="images/stride.jpg"
              width="180"
              height="120"
              style="display: block; margin: 0 auto; border-radius: 5px;"
              alt="STRIDER Image"
            />
          </td>
          <td class="pub_td2">
            <b>STRIDER: Navigation via Instruction-Aligned Structural Decision Space Optimization</b>
            <br /><br />
            <u>Diqi He</u>, <u>Xiangyu Gao</u>,
            <strong>Hao Li</strong>, <u>Junwei Han</u>, <u>Dingwen Zhang</u>
            <br />
            <div style="margin-top: 10px; margin-bottom: 10px">
              <i><strong style="color: brown">NeurIPS 2025</strong></i>
            </div>
            <a class="box" href="https://arxiv.org/abs/xxxx.xxxxx">
              <i class="fa fa-file-lines" aria-hidden="true">&nbsp;</i>Paper
            </a>
          </td>
        </tr>

        <tr id="langscenex2025">
          <td class="pub_td1">
            <img
              src="images/langscenex.png"
              width="180"
              height="120"
              style="display: block; margin: 0 auto; border-radius: 5px;"
              alt="LangScene-X Image"
            />
          </td>
          <td class="pub_td2">
            <b>LangScene-X: Reconstruct Generalizable 3D Language-Embedded Scenes with TriMap Video Diffusion</b>
            <br /><br />
            <u>Fangfu Liu†</u>,
            <strong>Hao Li†</strong>, <u>Junfeng Chi</u>, <u>Hailin Wang</u>, <u>Mingxi Yang</u>, <u>Fangyu Wang</u>, <u>Yachao Duan</u>
            <br />
            <div style="margin-top: 10px; margin-bottom: 10px">
              <i><strong style="color: brown">ICCV 2025</strong></i>
            </div>
            <a class="box" href="https://arxiv.org/abs/2506.xxxxx">
              <i class="fa fa-file-lines" aria-hidden="true">&nbsp;</i>Paper
            </a>
          </td>
        </tr>

        <tr id="citygs-x2025">
          <td class="pub_td1">
            <img
              src="images/CityGS-X.jpg"
              width="180"
              height="120"
              style="display: block; margin: 0 auto; border-radius: 5px;"
              alt="CityGS-X Image"
            />
          </td>
          <td class="pub_td2">
            <b>CityGS-X: A Scalable Architecture for Efficient and Geometrically Accurate Large-Scale Scene Reconstruction</b>
            <br /><br />
            <u>Yuanyuan Gao†</u>,
            <strong>Hao Li†</strong>, <u>Jiaqi Chen</u>, <u>Zhengyu Zou</u>, <u>Zhihang Zhong</u>, <u>Dingwen Zhang</u>, <u>Xiao Sun</u>, <u>Junwei Han</u>
            <br />
            <div style="margin-top: 10px; margin-bottom: 10px">
              <i><strong style="color: brown">ICCV 2025</strong></i>
            </div>
            <a class="box" href="https://arxiv.org/abs/2502.xxxxx">
              <i class="fa fa-file-lines" aria-hidden="true">&nbsp;</i>Paper
            </a>
          </td>
        </tr>

        <tr id="cosurfgs2024">
          <td class="pub_td1">
            <img
              src="images/cosurfgs_image.jpg"
              width="180"
              height="120"
              style="display: block; margin: 0 auto; border-radius: 5px;"
              alt="CoSurfGS Image"
            />
          </td>
          <td class="pub_td2">
            <b>CoSurfGS: Collaborative 3D Surface Gaussian Splatting with Distributed Learning for Large Scene Reconstruction</b>
            <br /><br />
            <u>Yuanyuan Gao†</u>, <u>Yalun Dai†</u>,
            <strong>Hao Li†</strong>, <u>Weicai Ye</u>, <u>Jiaqi Chen</u>, <u>Dingwen Chen</u>, <u>Dingwen Zhang</u>, <u>Tong He</u>, <u>Guofeng Zhang</u>, <u>Junwei Han</u>
            <br />
            <div style="margin-top: 10px; margin-bottom: 10px">
              <i><strong style="color: brown">IJCV 2024</strong></i>
            </div>
            <a class="box" href="https://arxiv.org/abs/2412.xxxxx">
              <i class="fa fa-file-lines" aria-hidden="true">&nbsp;</i>Paper
            </a>
          </td>
        </tr>

        <tr id="dgtr2024">
          <td class="pub_td1">
            <img
              src="images/dgtr_image.jpg"
              width="180"
              height="120"
              style="display: block; margin: 0 auto; border-radius: 5px;"
              alt="DGTR Image"
            />
          </td>
          <td class="pub_td2">
            <b>DGTR: Distributed Gaussian Turbo-Reconstruction for Sparse-View Vast Scene</b>
            <br /><br />
            <strong>Hao Li</strong>, <u>Yuanyuan Gao</u>, <u>Haoyi Peng</u>, <u>Chenming Wu</u>, <u>Weicai Ye</u>, <u>Yuxuan Zhan</u>, <u>Chen Zhao</u>, <u>Dingwen Zhang</u>, <u>Jingdong Wang</u>, <u>Junwei Han</u>
            <br />
            <div style="margin-top: 10px; margin-bottom: 10px">
              <i><strong style="color: brown">ICRA 2025</strong></i>
            </div>
            <a class="box" href="https://arxiv.org/abs/2411.xxxxx">
              <i class="fa fa-file-lines" aria-hidden="true">&nbsp;</i>Paper
            </a>
          </td>
        </tr>

        <tr id="li2024langsurf">
          <td class="pub_td1">
            <video autoplay loop muted width="180" height="120" style="display: block; margin: 0 auto;">
              <source src="./images/langsurf.mp4" type="video/mp4">
            </video>
            
          </td>
          <td class="pub_td2">
            <b
              >LangSurf: Language-Embedded Surface Gaussians for 3D Scene Understanding
              &nbsp;</b
            ><br />

            <br />
            <strong>Hao Li</strong>,
            <a href="https://qinminghan.github.io/">Minghan Qin#</a>,
            <u>Zhengyu Zou</u>,
            <u>Diqi He</u>,
            <u>Bohan Li</u>,
            <u>Bingquan Dai</u>,
            <u>Dingwen Zhang#</u>,
            <u>Junwei Han</u>
            <br />
            <div style="margin-top: 10px; margin-bottom: 10px">
              <i> <strong style="color: brown">arXiv 2024</strong></i>
            </div>
            <a class="box" href="https://langsurf.github.io/"
              ><i class="fas fa-home" aria-hidden="true">&nbsp;</i
              >Home&nbsp;</a>

            <a class="box" href="https://arxiv.org/abs/2412.17635"
              ><i class="fa fa-file-lines" aria-hidden="true">&nbsp;</i>Paper
            </a>

            <a class="box" href="https://youtu.be/jU83kljQaR0"
              ><i class="fa fa-video" aria-hidden="true">&nbsp;</i
              >Video&nbsp;</a>

            <a href="https://github.com/lifuguan/langsurf">
              <i class="fa fa-github" aria-hidden="true">&nbsp;</i>
              Code&nbsp;
            </a>

            <a
              shape="rect"
              href="javascript:togglebib(&#39;chen2024feat2gs&#39;)"
              class="togglebib box"
              ><i class="fas fa-quote-left" aria-hidden="true">&nbsp;</i
              >Bibtex&nbsp;</a
            >
            <pre
              id="feat2gsbib"
              xml:space="preserve"
              style="display: none"
            ></pre>
            <span
              id="feat2gsabs"
              style="display: none; margin-top: 10px"
            ></span>
          </td>
        </tr>

        <tr id="xld2024">
          <td class="pub_td1">
            <img
              src="images/xld_image.jpg"
              width="180"
              height="120"
              style="display: block; margin: 0 auto; border-radius: 5px;"
              alt="XLD Dataset Image"
            />
          </td>
          <td class="pub_td2">
            <b>XLD: A Cross-Lane Dataset for Benchmarking Novel Driving View Synthesis</b>
            <br /><br />
            <strong>Hao Li</strong>,
            <u>Chenming Wu</u>, <u>Chen Zhao</u>, <u>Haocheng Feng</u>, <u>Errui Ding</u>,
            <u>Dingwe Zhang#</u>, <u>Jingdong Wang</u>
            <br />
            <div style="margin-top: 10px; margin-bottom: 10px">
              <i><strong style="color: brown">3DV 2025</strong></i>
            </div>
            <a class="box" href="https://3d-aigc.github.io/XLD/"
              ><i class="fas fa-home" aria-hidden="true">&nbsp;</i>Home&nbsp;</a>
            <a class="box" href="https://arxiv.org/abs/2406.18360"
              ><i class="fa fa-file-lines" aria-hidden="true">&nbsp;</i>Paper</a>
              <a href="https://github.com/lifuguan/xld_code">
                <i class="fa fa-github" aria-hidden="true">&nbsp;</i>
                Code&nbsp;
              </a>
          </td>
        </tr>
        <tr id="vdg2024">
          <td class="pub_td1">
            <video autoplay loop muted width="180" height="120" style="display: block; margin: 0 auto;">
              <source src="images/vdg_video.mp4" type="video/mp4">
            </video>
          </td>
          <td class="pub_td2">
            <b>VDG: Vision-Only Dynamic Gaussian for Driving Simulation</b><br /><br />
            <strong>Hao Li</strong>,
            <u>Jingfeng Li</u>, <u>Dingwen Zhang</u>, <u>Chenming Wu</u>, <u>Jieqi Shi</u>, <u>Chen Zhao</u>, <u>Haocheng Feng</u>, <u>Errui Ding</u>, 
            <u>Jingdong Wang</u>, <u>Junwei Han</u><br />
            <div style="margin-top: 10px; margin-bottom: 10px">
              <i><strong style="color: brown">IEEE RA-L 2025</strong></i>
            </div>
            <a class="box" href="https://3d-aigc.github.io/VDG/">
              <i class="fas fa-home" aria-hidden="true">&nbsp;</i>Home&nbsp;
            </a>
            <a class="box" href="https://arxiv.org/abs/2406.18198">
              <i class="fa fa-file-lines" aria-hidden="true">&nbsp;</i>Paper
            </a>
          </td>
        </tr>
        
        <tr id="ggrt2024">
          <td class="pub_td1">
            <video autoplay loop muted width="180" height="120" style="display: block; margin: 0 auto;">
              <source src="images/ggrt_video.mp4" type="video/mp4">
            </video>
          </td>
          <td class="pub_td2">
            <b>GGRt: Towards Pose-free Generalizable 3D Gaussian Splatting in Real-time</b><br /><br />
            <strong>Hao Li</strong>,
            <u>Yuanyuan Gao</u>, <u>Chenming Wu</u>, <u>Dingwen Zhang</u>, <u>Yalun Dai</u>, <u>Chen Zhao</u>, <u>Haocheng Feng</u>, <u>Errui Ding</u>, 
            <u>Jingdong Wang</u>, <u>Junwei Han</u><br />
            <div style="margin-top: 10px; margin-bottom: 10px">
              <i><strong style="color: brown">ECCV 2024</strong></i>
            </div>
            <a class="box" href="https://3d-aigc.github.io/GGRt/">
              <i class="fas fa-home" aria-hidden="true">&nbsp;</i>Home&nbsp;
            </a> 
            <a class="box" href="https://arxiv.org/abs/2312.02981">
              <i class="fa fa-file-lines" aria-hidden="true">&nbsp;</i>Paper
            </a>
            <a href="https://github.com/lifuguan/GGRt_official">
              <i class="fa fa-github" aria-hidden="true">&nbsp;</i>Code&nbsp;
            </a>
          </td>
        </tr>
        
        <tr id="gpnerf2024">
          <td class="pub_td1">
            <video autoplay loop muted width="180" height="120" style="display: block; margin: 0 auto;">
              <source src="images/gpnerf_video.mp4" type="video/mp4">
            </video>
          </td>
          <td class="pub_td2">
            <b>GP-NeRF: Generalized Perception NeRF for Context-Aware 3D Scene Understanding</b><br /><br />
            <strong>Hao Li</strong>,
            <u>Dingwen Zhang</u>, <u>Yalun Dai</u>, <u>Nian Liu</u>, <u>Lechao Cheng</u>, <u>Jingfeng</u>, <u>Jingdong Wang</u>, <u>Junwei Han</u><br />
            <div style="margin-top: 10px; margin-bottom: 10px">
              <i><strong style="color: brown">CVPR 2024 - Highlight</strong></i>
            </div>
            <a class="box" href="https://lifuguan.github.io/gpnerf-pages/">
              <i class="fas fa-home" aria-hidden="true">&nbsp;</i>Home&nbsp;
            </a>
            <a class="box" href="https://arxiv.org/abs/2311.11863">
              <i class="fa fa-file-lines" aria-hidden="true">&nbsp;</i>Paper
            </a>
            <a href="https://github.com/lifuguan/GP-NeRF">
              <i class="fa fa-github" aria-hidden="true">&nbsp;</i>Code&nbsp;
            </a>
          </td>
        </tr>
        
        <tr id="ltgc2024">
          <td class="pub_td1">
            <img
              src="images/ltgc_image.jpg"
              width="180"
              height="120"
              style="display: block; margin: 0 auto; border-radius: 5px;"
              alt="LTGC Image"
            />
          </td>
          <td class="pub_td2">
            <b>LTGC: Long-Tail Recognition via Leveraging LLMs-driven Generated Content</b><br /><br />
            
            <u>Qihao Zhao†</u>, <u>Yalun Dai†</u>, <strong>Hao Li†</strong>, <u>Wei Hu</u>, <u>Fan Zhang</u>, <u>Jun Liu</u><br />
            <div style="margin-top: 10px; margin-bottom: 10px">
              <i><strong style="color: brown">CVPR 2024 - Oral Presentation</strong></i>
            </div>
            <a class="box" href="https://ltgccode.github.io/">
              <i class="fas fa-home" aria-hidden="true">&nbsp;</i>Home&nbsp;
            </a>
            <a class="box" href="https://arxiv.org/abs/2403.05854">
              <i class="fa fa-file-lines" aria-hidden="true">&nbsp;</i>Paper
            </a>
            <a href="https://github.com/ltgccode/LTGC-Long-Tail-Recognition-via-Leveraging-Generated-Content">
              <i class="fa fa-github" aria-hidden="true">&nbsp;</i>Code&nbsp;
            </a>
          </td>
        </tr>
        
        <tr id="asdt2024">
          <td class="pub_td1">
            <img
              src="images/asdt_image.jpg"
              width="180"
              height="120"
              style="display: block; margin: 0 auto; border-radius: 5px;"
              alt="ASDT Image"
            />
          </td>
          <td class="pub_td2">
            <b>Weakly Supervised Semantic Segmentation via Alternate Self-Dual Teaching</b><br /><br />
            <strong>Hao Li</strong>,
            <u>Dingwen Zhang</u>, <u>Chaowei Fang</u>, <u>Lechao Cheng</u>, <u>Mingming Cheng</u>, <u>Junwei Han</u><br />
            <div style="margin-top: 10px; margin-bottom: 10px">
              <i><strong style="color: brown">IEEE TIP 2024</strong></i>
            </div>
            <a class="box" href="https://ieeexplore.ieee.org/document/10367821">
              <i class="fas fa-file-lines" aria-hidden="true">&nbsp;</i>Home&nbsp;
            </a>
            <a href="https://github.com/lifuguan/ASDT">
              <i class="fa fa-github" aria-hidden="true">&nbsp;</i>Code&nbsp;
            </a>
          </td>
        </tr>

        <tr id="tpami2024">
          <td class="pub_td1">
            <img
              src="images/lvp_image.jpg"
              width="180"
              height="120"
              style="display: block; margin: 0 auto; border-radius: 5px;"
              alt="LVP Image"
            />
          </td>
          <td class="pub_td2">
            <b>Unsupervised Pre-training with Language-Vision Prompts for Low-Data Instance Segmentation</b><br /><br />
            <u>Dingwen Zhang</u>,
            <strong>Hao Li</strong>, <u>Diqi He</u>, <u>Nian Liu</u>, <u>Lechao Cheng</u>, <u>Jingdong Wang</u>, <u>Junwei Han</u><br />
            <div style="margin-top: 10px; margin-bottom: 10px">
              <i><strong style="color: brown">IEEE TPAMI 2024</strong></i>
            </div>
            <a class="box" href="https://ieeexplore.ieee.org/document/xxxxx">
              <i class="fas fa-file-lines" aria-hidden="true">&nbsp;</i>Paper&nbsp;
            </a>
          </td>
        </tr>
        
        <tr id="saliencyprompt2023">
          <td class="pub_td1">
            <img
              src="images/boost_image.jpg"
              width="180"
              height="120"
              style="display: block; margin: 0 auto; border-radius: 5px;"
              alt="Saliency Prompt Image"
            />
          </td>
          <td class="pub_td2">
            <b>Boosting low-data instance segmentation by unsupervised pre-training with saliency prompt</b><br /><br />
            <strong>Hao Li</strong>,
            <u>Dingwen Zhang</u>, <u>Nian Liu</u>, <u>Lechao Cheng</u>, <u>Yalun Dai</u>, <u>Xinggang</u>, <u>Junwei Han</u><br />
            <div style="margin-top: 10px; margin-bottom: 10px">
              <i><strong style="color: brown">CVPR 2023</strong></i>
            </div>
            <a class="box" href="https://arxiv.org/abs/2302.01171">
              <i class="fas fa-file-lines" aria-hidden="true">&nbsp;</i>Home&nbsp;
            </a>
          </td>
        </tr>
        
      </table>

      <h1>Talks</h1>
      <li>可泛化3DGS的现状和未来  (<a href="https://mp.weixin.qq.com/s/rGh-zI0zYaJKZ0EWIjyy7w">GAMES Webinar</a>)</li>
      <li>可泛化语义NeRF  (<a href="https://mp.weixin.qq.com/s/htNrZaDuhwez03kqJLwjpg">3D视觉工坊</a>)</li>

      <h1>Honors and Awards</h1>
      <li> <b>Outstanding Students (First Grade)</b> in Northwestern Polytechnical University, 2024.</li>
      <li> <b>Outstanding Interns</b> in Baidu Inc., 2023.</li>
      <li> <b>2nd</b> in International Algorithm Case Competition, 2022.</li>
      <li> <b>Finalist Winner (2%)</b> in International Mathematical Contest in Modeling (MCM), 2021.</li>
      <br />
      

      
      <div align="center">
        <div style="width: 250px; height: 250px">
          <!-- <script type="text/javascript" id="clstr_globe"
            src="//clustrmaps.com/globe.js?d=rW3OE8b2mKOjjq_alCSlCohNv0gBVFF6R0Pj4W60HmY"></script> -->
          <script type="text/javascript" src="//rf.revolvermaps.com/0/0/8.js?i=5pkd9xbyiaa&amp;m=0&amp;c=00fff6&amp;cr1=ffffff&amp;f=arial&amp;l=33" async="async"></script>
        </div>
        The source code of this website is borrowed from <a href="https://xiuyuliang.cn/">Yuliang Xiu</a> <br />
        <strong>
          The page is last updated at <span style="color: rgb(53, 182, 171)" id="demo"></span>
        </strong>
      </div>
    </div>
  </div>
</body>

</html>
